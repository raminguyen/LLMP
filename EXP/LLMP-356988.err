The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:05<00:20,  5.11s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:09<00:14,  4.85s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:14<00:09,  4.84s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:19<00:04,  4.91s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:21<00:00,  3.64s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:21<00:00,  4.20s/it]
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/home/huuthanhvy.nguyen001/anaconda3/envs/sbatch2/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/42 [00:00<?, ?it/s]Keyword argument `padding` is not a valid argument for this processor and will be ignored.
/home/huuthanhvy.nguyen001/anaconda3/envs/sbatch2/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/conda/conda-bld/pytorch_1728929546833/work/aten/src/ATen/native/cudnn/MHA.cpp:674.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  2%|▏         | 1/42 [00:12<08:21, 12.24s/it]  5%|▍         | 2/42 [00:19<06:20,  9.51s/it]  7%|▋         | 3/42 [00:27<05:34,  8.57s/it] 10%|▉         | 4/42 [00:33<04:49,  7.61s/it] 12%|█▏        | 5/42 [00:39<04:21,  7.08s/it] 14%|█▍        | 6/42 [00:44<03:46,  6.30s/it] 17%|█▋        | 7/42 [00:49<03:23,  5.81s/it] 19%|█▉        | 8/42 [00:53<03:06,  5.49s/it] 21%|██▏       | 9/42 [00:58<02:54,  5.30s/it] 24%|██▍       | 10/42 [01:04<02:57,  5.55s/it] 26%|██▌       | 11/42 [01:09<02:44,  5.32s/it] 29%|██▊       | 12/42 [01:14<02:34,  5.16s/it] 31%|███       | 13/42 [01:19<02:26,  5.05s/it] 33%|███▎      | 14/42 [01:24<02:19,  4.97s/it] 36%|███▌      | 15/42 [01:28<02:12,  4.92s/it] 38%|███▊      | 16/42 [01:33<02:07,  4.90s/it] 40%|████      | 17/42 [01:38<02:01,  4.87s/it] 43%|████▎     | 18/42 [01:43<01:56,  4.84s/it] 45%|████▌     | 19/42 [01:48<01:51,  4.83s/it] 48%|████▊     | 20/42 [01:52<01:46,  4.82s/it] 50%|█████     | 21/42 [01:57<01:41,  4.81s/it] 52%|█████▏    | 22/42 [02:02<01:36,  4.80s/it] 55%|█████▍    | 23/42 [02:07<01:32,  4.87s/it] 57%|█████▋    | 24/42 [02:12<01:27,  4.84s/it] 60%|█████▉    | 25/42 [02:17<01:22,  4.83s/it] 62%|██████▏   | 26/42 [02:21<01:17,  4.81s/it] 64%|██████▍   | 27/42 [02:26<01:12,  4.81s/it] 67%|██████▋   | 28/42 [02:31<01:07,  4.80s/it] 69%|██████▉   | 29/42 [02:36<01:02,  4.80s/it] 71%|███████▏  | 30/42 [02:41<00:57,  4.82s/it] 74%|███████▍  | 31/42 [02:45<00:52,  4.81s/it] 76%|███████▌  | 32/42 [02:50<00:48,  4.81s/it] 79%|███████▊  | 33/42 [02:55<00:43,  4.80s/it] 81%|████████  | 34/42 [03:00<00:38,  4.80s/it] 83%|████████▎ | 35/42 [03:05<00:33,  4.80s/it] 86%|████████▌ | 36/42 [03:09<00:28,  4.79s/it] 88%|████████▊ | 37/42 [03:14<00:23,  4.79s/it] 90%|█████████ | 38/42 [03:19<00:19,  4.80s/it] 93%|█████████▎| 39/42 [03:24<00:14,  4.79s/it] 95%|█████████▌| 40/42 [03:29<00:09,  4.79s/it] 98%|█████████▊| 41/42 [03:33<00:04,  4.79s/it]100%|██████████| 42/42 [03:38<00:00,  4.79s/it]                                               100%|██████████| 42/42 [03:38<00:00,  4.79s/it]100%|██████████| 42/42 [03:38<00:00,  5.20s/it]
