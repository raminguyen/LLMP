{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cb8796-f27c-4581-abf5-22451671e010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import png\n",
    "import io\n",
    "import numpy as np\n",
    "import base64\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForPreTraining, TrainingArguments, Trainer, MllamaForConditionalGeneration, AutoProcessor, BitsAndBytesConfig, AutoModelForVision2Seq, pipeline, AutoModel\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from PIL import Image\n",
    "\n",
    "class llama:\n",
    "    \n",
    "    def __init__(self, model_name):\n",
    "        print(f\"Initializing llamaModel with model_name: {model_name}\")\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def query(self, question, image=None):\n",
    "\n",
    "        size = image.shape[0]\n",
    "        grayscale = np.zeros((size,size), dtype=np.uint8)\n",
    "        grayscale[image==0] = 255\n",
    "        grayscale[image==1] = 0\n",
    "\n",
    "        pil_image = Image.fromarray(grayscale)\n",
    "\n",
    "        # Bits and Bytes configuration\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "        # Model and processor loading\n",
    "        model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "        model = AutoModelForPreTraining.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",   # Automatically map model to available devices (e.g., GPU)\n",
    "            torch_dtype=torch.bfloat16,  # Using bfloat16 for reduced memory usage\n",
    "            quantization_config=bnb_config,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "\n",
    "        image = pil_image.convert(\"RGB\")\n",
    "\n",
    "\n",
    "        processor = AutoProcessor.from_pretrained(self.model_name)\n",
    "\n",
    "        model.tie_weights()\n",
    "        text = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n<|image|>{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "        # For inference, you only need to process the inputs, without handling labels or setting -100\n",
    "        batch = processor(text=text, images=image, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "        \n",
    "        # Run inference on the model with the preprocessed batch\n",
    "        outputs = model.generate(**batch, max_length=50)\n",
    "        \n",
    "        # Decode the generated output\n",
    "        decoded_output = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "        return decoded_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
