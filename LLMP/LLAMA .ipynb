{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert a train/validation/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 4608, Validation size: 1152, Test size: 1440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Applying K-Fold Cross Validation on the training set\\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\\n\\n# Iterate through the splits\\nfor fold, (train_index, val_index) in enumerate(kf.split(train)):\\n    train_fold = train.iloc[train_index]\\n    val_fold = train.iloc[val_index]\\n    \\n    # Save each fold\\'s train and validation set to JSON files\\n    train_fold.to_json(f\\'train_fold_{fold}.json\\', orient=\\'records\\', indent=4)\\n    val_fold.to_json(f\\'val_fold_{fold}.json\\', orient=\\'records\\', indent=4)\\n    \\n    print(f\"Fold {fold + 1}: Train size: {len(train_fold)}, Validation size: {len(val_fold)}\")\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import json\n",
    "\n",
    "# Load the dataset.json into a pandas DataFrame\n",
    "df = pd.read_json('dataset_7200.json')\n",
    "\n",
    "# Split dataset into train+val (80%) and test (20%)\n",
    "train_val, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split train+val into train (80% of 80% = 64% of total) and val (20% of 80% = 16% of total)\n",
    "train, val = train_test_split(train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the train, val, and test datasets as separate JSON files\n",
    "train.to_json('train_dataset.json', orient='records', indent=4)\n",
    "val.to_json('val_dataset.json', orient='records', indent=4)\n",
    "test.to_json('test_dataset.json', orient='records', indent=4)\n",
    "\n",
    "print(f\"Train size: {len(train)}, Validation size: {len(val)}, Test size: {len(test)}\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Applying K-Fold Cross Validation on the training set\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate through the splits\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(train)):\n",
    "    train_fold = train.iloc[train_index]\n",
    "    val_fold = train.iloc[val_index]\n",
    "    \n",
    "    # Save each fold's train and validation set to JSON files\n",
    "    train_fold.to_json(f'train_fold_{fold}.json', orient='records', indent=4)\n",
    "    val_fold.to_json(f'val_fold_{fold}.json', orient='records', indent=4)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: Train size: {len(train_fold)}, Validation size: {len(val_fold)}\")\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 21 08:17:51 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   27C    P0              59W / 400W |      0MiB / 40960MiB |     29%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kill -9 2091683\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'image', 'question', 'value'],\n",
      "    num_rows: 4608\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'image', 'question', 'value'],\n",
      "    num_rows: 1152\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'image', 'question', 'value'],\n",
      "    num_rows: 1440\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Assuming 'train' is a pandas DataFrame\n",
    "\n",
    "# Create a list of dictionaries from the DataFrame\n",
    "train_dataset = [\n",
    "    {\n",
    "        'id': row['id'],          # Extract the 'id' column\n",
    "        'image': row['image'],    # Extract the 'image' column\n",
    "        'question': \"What is the exact size of this acute angle?\",  # Add the question for the prompt\n",
    "        'value': row['angle']     # Extract the 'angle' column as the 'value'\n",
    "    }\n",
    "    for _, row in train.iterrows()  # Iterate over DataFrame rows\n",
    "]\n",
    "\n",
    "# Convert the list of dictionaries into a Hugging Face Dataset\n",
    "train_data = Dataset.from_list(train_dataset)\n",
    "\n",
    "# Print the dataset structure for verification\n",
    "print(train_data)\n",
    "\n",
    "# Assuming 'val' is your pandas DataFrame for validation data\n",
    "\n",
    "# Create a list of dictionaries from the DataFrame\n",
    "validation_dataset = [\n",
    "    {\n",
    "        'id': row['id'],          # Extract the 'id' column\n",
    "        'image': row['image'],    # Extract the 'image' column\n",
    "        'question': \"What is the exact size of this acute angle?\",  # Add the question for validation\n",
    "        'value': row['angle']     # Extract the 'angle' column as the 'value'\n",
    "    }\n",
    "    for _, row in val.iterrows()  # Iterate over DataFrame rows\n",
    "]\n",
    "\n",
    "# Convert the list of dictionaries into a Hugging Face Dataset\n",
    "validation_data = Dataset.from_list(validation_dataset)\n",
    "\n",
    "# Print the dataset structure for verification\n",
    "print(validation_data)\n",
    "\n",
    "# Assuming 'test' is your pandas DataFrame for test data\n",
    "\n",
    "# Create a list of dictionaries from the DataFrame\n",
    "test_dataset = [\n",
    "    {\n",
    "        'id': row['id'],          # Extract the 'id' column\n",
    "        'image': row['image'],    # Extract the 'image' column\n",
    "        'question': \"What is the exact size of this acute angle?\",  # Add the question for test\n",
    "        'value': row['angle']     # Extract the 'angle' column as the 'value'\n",
    "    }\n",
    "    for _, row in test.iterrows()  # Iterate over DataFrame rows\n",
    "]\n",
    "\n",
    "# Convert the list of dictionaries into a Hugging Face Dataset\n",
    "test_data = Dataset.from_list(test_dataset)\n",
    "\n",
    "# Print the dataset structure for verification\n",
    "print(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12c7bbe25b04755852a990faad10898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/huuthanhvy.nguyen001/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f96f5e5bdca40d683e8a804124f1096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, Trainer, TrainingArguments, TrainerCallback\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "# Define the model ID and login\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "login('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')\n",
    "\n",
    "# Load Bits and Bytes Configuration for quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_use_double_quant=True, \n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load the model with quantization\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Load the processor\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Define LoRA config based on QLoRA experiments\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    r=32,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # LoRA targets these transformer layers\n",
    "    task_type=\"FEATURE_EXTRACTION\",  # Task type for Feature_extration\n",
    ")\n",
    "\n",
    "model.tie_weights() \n",
    "\n",
    "# Apply LoRA adapters to the loaded model\n",
    "model = get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 1348118\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing the images\n",
    "image_folder = \"/home/huuthanhvy.nguyen001/hpcstor6/LLMP/LLMP/LLMP/generated_images_7200\"\n",
    "\n",
    "# Function to process the examples\n",
    "def process(examples):\n",
    "    # Construct the prompt asking for the angle in the image\n",
    "    texts = [\n",
    "        f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n<|image|> What is the exact size of this acute angle? <|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{item['value']}<|eot_id|>\"\n",
    "        for item in examples\n",
    "    ]\n",
    "    \n",
    "    # Load images from the folder\n",
    "    images = [\n",
    "        Image.open(os.path.join(image_folder, item[\"image\"])).convert(\"RGB\")\n",
    "        for item in examples\n",
    "    ]\n",
    "\n",
    "    # Assuming `processor` is defined elsewhere in the code\n",
    "    # The processor will handle the tokenization of the text and processing of the image\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Clone the input IDs to create labels, masking padding and image token index\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == 128256] = -100  # Mask image token index for images\n",
    "\n",
    "    # Assign the correct output (angle value) as the label for each example\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    # Move the batch to bfloat16 and to the GPU (cuda) for faster training\n",
    "    batch = batch.to(torch.bfloat16).to(\"cuda\")\n",
    "    \n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/huuthanhvy.nguyen001/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 08:21:40.824259: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-21 08:21:40.840438: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-21 08:21:40.845036: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-21 08:21:40.856370: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-21 08:21:41.918191: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-21 08:21:43,371] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuthanhvy.nguyen001/anaconda3/envs/pytorch/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/huuthanhvy.nguyen001/anaconda3/envs/pytorch/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/huuthanhvy.nguyen001/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuthanhvy.nguyen001/anaconda3/envs/pytorch/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrami-nguyen12\u001b[0m (\u001b[33mramihuunguyen\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/huuthanhvy.nguyen001/hpcstor6/LLMP/LLMP/LLMP/wandb/run-20241021_082145-znv9f51p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ramihuunguyen/huggingface/runs/znv9f51p' target=\"_blank\">generated_images_7200</a></strong> to <a href='https://wandb.ai/ramihuunguyen/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ramihuunguyen/huggingface' target=\"_blank\">https://wandb.ai/ramihuunguyen/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ramihuunguyen/huggingface/runs/znv9f51p' target=\"_blank\">https://wandb.ai/ramihuunguyen/huggingface/runs/znv9f51p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='439' max='3456' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 439/3456 39:41 < 4:33:58, 0.18 it/s, Epoch 0.38/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>10.394800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.892800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.802900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.762400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Define the model ID and login\n",
    "login('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')\n",
    "\n",
    "# Dynamically set output_dir based on the number of images\n",
    "num_images = 7200  # Change this number as needed\n",
    "output_dir = f\"generated_images_{num_images}\"  # E.g., \"generated_images_450\"\n",
    "log_file_path = f\"training_logs_{num_images}.txt\"  # Dynamic log file path\n",
    "\n",
    "# Modify TrainingArguments to include evaluation strategy\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,  # Updated name here\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=100,\n",
    "    remove_unused_columns=False,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,  # Add batch size for evaluation\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=100,\n",
    "    learning_rate=0.0001,\n",
    "    weight_decay=0.01,\n",
    "    adam_beta2=0.999,\n",
    "    max_grad_norm=1.0,\n",
    "    save_strategy=\"no\",\n",
    "    optim=\"adamw_hf\",\n",
    "    save_total_limit=1,\n",
    "    bf16=True,\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "# Custom callback to log both training and validation metrics\n",
    "class LogMetricsCallback(TrainerCallback):\n",
    "    def __init__(self, log_file_path):\n",
    "        self.log_file_path = log_file_path\n",
    "        self.training_logs = []\n",
    "        self.validation_logs = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            # Write logs to file\n",
    "            with open(self.log_file_path, \"a\") as f:\n",
    "                if \"loss\" in logs:  # Log training loss\n",
    "                    self.training_logs.append((state.global_step, logs[\"loss\"]))\n",
    "                    f.write(f\"Training loss: {logs['loss']} at step {state.global_step}\\n\")\n",
    "                \n",
    "                if \"eval_loss\" in logs:  # Log validation loss\n",
    "                    self.validation_logs.append((state.global_step, logs[\"eval_loss\"]))\n",
    "                    f.write(f\"Validation loss: {logs['eval_loss']} at step {state.global_step}\\n\")\n",
    "                    print(f\"Validation loss: {logs['eval_loss']} at step {state.global_step}\")\n",
    "\n",
    "# Initialize the custom callback with the log file path\n",
    "log_metrics_callback = LogMetricsCallback(log_file_path=log_file_path)\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Assuming `model` is loaded before this section\n",
    "model.tie_weights()  # Tie the weights after loading the model\n",
    "\n",
    "# Trainer setup including validation data\n",
    "trainer = Trainer(\n",
    "    model=model,  # Ensure 'model' is defined and initialized\n",
    "    args=training_args,\n",
    "    data_collator=process,  # Assuming 'process' is defined and appropriate\n",
    "    train_dataset=train_data,  # Assuming 'train_data' is defined\n",
    "    eval_dataset=validation_data,  # Add validation dataset\n",
    "    callbacks=[log_metrics_callback],\n",
    ")\n",
    "\n",
    "# Train the model with validation\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"my_finetuned_llama_7200_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct 19 19:31:26 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   25C    P0              56W / 400W |  22783MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   1163882      C   ...1/anaconda3/envs/pytorch/bin/python    22774MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Push model to the hub after training\n",
    "#trainer.push_to_hub()\n",
    "\n",
    "model.save_pretrained(\"my_finetuned_llama\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForFeatureExtraction(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MllamaForConditionalGeneration(\n",
      "      (vision_model): MllamaVisionModel(\n",
      "        (patch_embedding): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14), padding=valid, bias=False)\n",
      "        (gated_positional_embedding): MllamaPrecomputedPositionEmbedding(\n",
      "          (tile_embedding): Embedding(9, 8197120)\n",
      "        )\n",
      "        (pre_tile_positional_embedding): MllamaPrecomputedAspectRatioEmbedding(\n",
      "          (embedding): Embedding(9, 5120)\n",
      "        )\n",
      "        (post_tile_positional_embedding): MllamaPrecomputedAspectRatioEmbedding(\n",
      "          (embedding): Embedding(9, 5120)\n",
      "        )\n",
      "        (layernorm_pre): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        (layernorm_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        (transformer): MllamaVisionEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-31): 32 x MllamaVisionEncoderLayer(\n",
      "              (self_attn): MllamaVisionSdpaAttention(\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1280, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=1280, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1280, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=1280, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (o_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n",
      "              )\n",
      "              (mlp): MllamaVisionMLP(\n",
      "                (activation_fn): GELUActivation()\n",
      "                (fc1): Linear4bit(in_features=1280, out_features=5120, bias=True)\n",
      "                (fc2): Linear4bit(in_features=5120, out_features=1280, bias=True)\n",
      "              )\n",
      "              (input_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (post_attention_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (global_transformer): MllamaVisionEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-7): 8 x MllamaVisionEncoderLayer(\n",
      "              (self_attn): MllamaVisionSdpaAttention(\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1280, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=1280, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1280, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=1280, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (o_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n",
      "              )\n",
      "              (mlp): MllamaVisionMLP(\n",
      "                (activation_fn): GELUActivation()\n",
      "                (fc1): Linear4bit(in_features=1280, out_features=5120, bias=True)\n",
      "                (fc2): Linear4bit(in_features=5120, out_features=1280, bias=True)\n",
      "              )\n",
      "              (input_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (post_attention_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (language_model): MllamaForCausalLM(\n",
      "        (model): MllamaTextModel(\n",
      "          (embed_tokens): Embedding(128264, 4096, padding_idx=128004)\n",
      "          (layers): ModuleList(\n",
      "            (0-2): 3 x MllamaSelfAttentionDecoderLayer(\n",
      "              (self_attn): MllamaTextSelfSdpaAttention(\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              )\n",
      "              (mlp): MllamaTextMLP(\n",
      "                (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "            )\n",
      "            (3): MllamaCrossAttentionDecoderLayer(\n",
      "              (cross_attn): MllamaTextCrossSdpaAttention(\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "                (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "              )\n",
      "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "              (mlp): MllamaTextMLP(\n",
      "                (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "            )\n",
      "            (4-7): 4 x MllamaSelfAttentionDecoderLayer(\n",
      "              (self_attn): MllamaTextSelfSdpaAttention(\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              )\n",
      "              (mlp): MllamaTextMLP(\n",
      "                (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "            )\n",
      "            (8): MllamaCrossAttentionDecoderLayer(\n",
      "              (cross_attn): MllamaTextCrossSdpaAttention(\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "                (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "              )\n",
      "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "              (mlp): MllamaTextMLP(\n",
      "                (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "            )\n",
      "            (9-12): 4 x MllamaSelfAttentionDecoderLayer(\n",
      "              (self_attn): MllamaTextSelfSdpaAttention(\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              )\n",
      "              (mlp): MllamaTextMLP(\n",
      "                (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "            )\n",
      "            (13): MllamaCrossAttentionDecoderLayer(\n",
      "              (cross_attn): MllamaTextCrossSdpaAttention(\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "                (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "              )\n",
      "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "              (mlp): MllamaTextMLP(\n",
      "                (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "            )\n",
      "            (14-17): 4 x MllamaSelfAttentionDecoderLayer(\n",
      "              (self_attn): MllamaTextSelfSdpaAttention(\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              )\n",
      "              (mlp): MllamaTextMLP(\n",
      "                (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "            )\n",
      "            (18): MllamaCrossAttentionDecoderLayer(\n",
      "              (cross_attn): MllamaTextCrossSdpaAttention(\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "                (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "              )\n",
      "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "              (mlp): MllamaTextMLP(\n",
      "                (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "            )\n",
      "            (19-22): 4 x MllamaSelfAttentionDecoderLayer(\n",
      "              (self_attn): MllamaTextSelfSdpaAttention(\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              )\n",
      "              (mlp): MllamaTextMLP(\n",
      "                (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "            )\n",
      "            (23): MllamaCrossAttentionDecoderLayer(\n",
      "              (cross_attn): MllamaTextCrossSdpaAttention(\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "                (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "              )\n",
      "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "              (mlp): MllamaTextMLP(\n",
      "                (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "            )\n",
      "            (24-27): 4 x MllamaSelfAttentionDecoderLayer(\n",
      "              (self_attn): MllamaTextSelfSdpaAttention(\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              )\n",
      "              (mlp): MllamaTextMLP(\n",
      "                (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "            )\n",
      "            (28): MllamaCrossAttentionDecoderLayer(\n",
      "              (cross_attn): MllamaTextCrossSdpaAttention(\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "                (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "              )\n",
      "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "              (mlp): MllamaTextMLP(\n",
      "                (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "            )\n",
      "            (29-32): 4 x MllamaSelfAttentionDecoderLayer(\n",
      "              (self_attn): MllamaTextSelfSdpaAttention(\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              )\n",
      "              (mlp): MllamaTextMLP(\n",
      "                (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "            )\n",
      "            (33): MllamaCrossAttentionDecoderLayer(\n",
      "              (cross_attn): MllamaTextCrossSdpaAttention(\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "                (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "              )\n",
      "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "              (mlp): MllamaTextMLP(\n",
      "                (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "            )\n",
      "            (34-37): 4 x MllamaSelfAttentionDecoderLayer(\n",
      "              (self_attn): MllamaTextSelfSdpaAttention(\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              )\n",
      "              (mlp): MllamaTextMLP(\n",
      "                (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "            )\n",
      "            (38): MllamaCrossAttentionDecoderLayer(\n",
      "              (cross_attn): MllamaTextCrossSdpaAttention(\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "                (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
      "              )\n",
      "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "              (mlp): MllamaTextMLP(\n",
      "                (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "            )\n",
      "            (39): MllamaSelfAttentionDecoderLayer(\n",
      "              (self_attn): MllamaTextSelfSdpaAttention(\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              )\n",
      "              (mlp): MllamaTextMLP(\n",
      "                (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "              (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "              (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "            )\n",
      "          )\n",
      "          (norm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
      "          (rotary_emb): MllamaRotaryEmbedding()\n",
      "        )\n",
      "        (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      "      )\n",
      "      (multi_modal_projector): Linear4bit(in_features=7680, out_features=4096, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to hub if needed\n",
    "# Save the model and tokenizer\n",
    "output_dir = \"./my_model_llama\"\n",
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training value: 77\n",
      "Model response: user\n",
      "\n",
      "what is exact size of angle degree?assistant\n",
      "\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Add LLMP module to system path\n",
    "sys.path.append('/home/huuthanhvy.nguyen001/hpcstor6/LLMP/LLMP/')\n",
    "from LLMP import GPImage  # Import GPImage class from LLMP\n",
    "\n",
    "# Generate the image with a preset 45-degree angle\n",
    "image, label = GPImage.figure1(\"angle\", preset=77)\n",
    "\n",
    "# Prepare your input data\n",
    "input_question = \"what is exact size of angle degree?\"\n",
    "print(f\"Training value: {label}\")  # Updated to use the generated label\n",
    "\n",
    "# Prepare the text input in the format required\n",
    "text = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n<|image|>{input_question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "# Use the generated image for inference\n",
    "# Convert the image (numpy array) to a PIL Image\n",
    "image_pil = Image.fromarray(image)\n",
    "\n",
    "# Convert the image to RGB format (if necessary)\n",
    "\n",
    "image_rgb = image_pil.convert(\"RGB\")\n",
    "\n",
    "# Process the inputs (text and image)\n",
    "inputs = processor(images=image_rgb, text=text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Inference: Generate predictions using the model\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_length=50)\n",
    "\n",
    "# Decode and print the output\n",
    "response = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(\"Model response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Add LLMP module to system path\n",
    "sys.path.append('/home/huuthanhvy.nguyen001/hpcstor6/LLMP/LLMP/')\n",
    "from LLMP import GPImage  # Import GPImage class from LLMP\n",
    "\n",
    "# Generate the image with a preset 45-degree angle\n",
    "image, label = GPImage.figure1(\"angle\", preset=120)\n",
    "\n",
    "# Plot the generated image using matplotlib\n",
    "plt.imshow(image, cmap='gray')  # Display the binary image as grayscale\n",
    "plt.title(f'Angle: {label} degrees')\n",
    "plt.show()\n",
    "\n",
    "# 2. Convert the image (numpy array) to a PIL Image\n",
    "image_pil = Image.fromarray(image)\n",
    "\n",
    "# Convert the image to RGB format (if necessary)\n",
    "image_rgb = image_pil.convert(\"RGB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
